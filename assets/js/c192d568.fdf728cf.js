"use strict";(self.webpackChunkcreate_project_docs=self.webpackChunkcreate_project_docs||[]).push([[2557],{28453:(e,t,n)=>{n.d(t,{R:()=>s,x:()=>r});var i=n(96540);const o={},a=i.createContext(o);function s(e){const t=i.useContext(a);return i.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),i.createElement(a.Provider,{value:t},e.children)}},73073:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>d,contentTitle:()=>r,default:()=>h,frontMatter:()=>s,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"system-architecture/algorithms","title":"Algorithm Overview","description":"The project leverages a variety of algorithms to process videos, analyze frames, and generate meaningful questions based on those analyses. The core components are YOLOv7 for object detection, machine learning for analyzing frames, OpenAI\u2019s API for generating questions from those analyses, and Google\u2019s Text-to-Speech API for reading the questions aloud. The training data is sourced from videos, primarily YouTube Kids, which are used for both training and testing the model. Below is an overview of the key algorithms employed in the project:","source":"@site/docs/system-architecture/algorithms.md","sourceDirName":"system-architecture","slug":"/system-architecture/algorithms","permalink":"/project-piggyback-learning-team-2/docs/system-architecture/algorithms","draft":false,"unlisted":false,"editUrl":"https://github.com/Capstone-Projects-2025-Spring/project-piggyback-learning-team-2/edit/main/documentation/docs/system-architecture/algorithms.md","tags":[],"version":"current","lastUpdatedBy":"TonyGao777","sidebarPosition":8,"frontMatter":{"sidebar_position":8},"sidebar":"docsSidebar","previous":{"title":"Version Control","permalink":"/project-piggyback-learning-team-2/docs/system-architecture/version-control"},"next":{"title":"API Specification","permalink":"/project-piggyback-learning-team-2/docs/category/api-specification"}}');var o=n(74848),a=n(28453);const s={sidebar_position:8},r="Algorithm Overview",d={},c=[{value:"YOLOv7: Object Detection",id:"yolov7-object-detection",level:2},{value:"Training and Dataset for YOLOv7",id:"training-and-dataset-for-yolov7",level:3},{value:"Testing Data:",id:"testing-data",level:3},{value:"OpenAI API: Question Generation",id:"openai-api-question-generation",level:2},{value:"Text Generation:",id:"text-generation",level:3},{value:"Natural Language Understanding:",id:"natural-language-understanding",level:3},{value:"Google Text-to-Speech: Question Reading",id:"google-text-to-speech-question-reading",level:2},{value:"Search Algorithms",id:"search-algorithms",level:2}];function l(e){const t={h1:"h1",h2:"h2",h3:"h3",header:"header",p:"p",...(0,a.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(t.header,{children:(0,o.jsx)(t.h1,{id:"algorithm-overview",children:"Algorithm Overview"})}),"\n",(0,o.jsx)(t.p,{children:"The project leverages a variety of algorithms to process videos, analyze frames, and generate meaningful questions based on those analyses. The core components are YOLOv7 for object detection, machine learning for analyzing frames, OpenAI\u2019s API for generating questions from those analyses, and Google\u2019s Text-to-Speech API for reading the questions aloud. The training data is sourced from videos, primarily YouTube Kids, which are used for both training and testing the model. Below is an overview of the key algorithms employed in the project:"}),"\n",(0,o.jsx)(t.h2,{id:"yolov7-object-detection",children:"YOLOv7: Object Detection"}),"\n",(0,o.jsx)(t.p,{children:"YOLOv7 (You Only Look Once version 7) is a state-of-the-art object detection model, designed to identify and classify objects in images and videos in real time. YOLOv7 uses a single convolutional neural network (CNN) to predict the bounding boxes and class probabilities of objects. This allows the model to identify multiple objects within a frame with a high degree of accuracy and speed. YOLOv7 is particularly suitable for this project as it can detect objects efficiently from video frames, which is essential for creating the basis for the questions generated in the app."}),"\n",(0,o.jsx)(t.h3,{id:"training-and-dataset-for-yolov7",children:"Training and Dataset for YOLOv7"}),"\n",(0,o.jsx)(t.p,{children:"The dataset consists of videos sourced from YouTube Kids. These videos are labeled manually to identify the objects that YOLOv7 should learn to recognize. The dataset includes diverse video content (real and animated) that helps the model generalize to different scenarios and objects."}),"\n",(0,o.jsx)(t.h3,{id:"testing-data",children:"Testing Data:"}),"\n",(0,o.jsx)(t.p,{children:"Separate video data, also from YouTube Kids, is used to test the model. The performance of YOLOv7 is evaluated based on its ability to accurately detect objects and the speed at which it processes frames."}),"\n",(0,o.jsx)(t.h2,{id:"openai-api-question-generation",children:"OpenAI API: Question Generation"}),"\n",(0,o.jsx)(t.p,{children:"Once YOLOv7 identifies the objects in a video frame, the next step involves generating educational questions based on those objects. This is where OpenAI\u2019s API comes in. The API is used to generate natural language questions from the detected objects and their context."}),"\n",(0,o.jsx)(t.h3,{id:"text-generation",children:"Text Generation:"}),"\n",(0,o.jsx)(t.p,{children:"OpenAI\u2019s GPT model is employed to process the information about the detected objects and frame context (such as the number of objects, types, and relationships) to generate questions that are engaging and relevant to the video content."}),"\n",(0,o.jsx)(t.h3,{id:"natural-language-understanding",children:"Natural Language Understanding:"}),"\n",(0,o.jsx)(t.p,{children:'The system is designed to understand the relationship between detected objects and generate questions that challenge the user\u2019s understanding. For instance, if the object detected is a ball, the system might ask, "What is the name of the object in the video?" or "How many balls are there?"'}),"\n",(0,o.jsx)(t.h2,{id:"google-text-to-speech-question-reading",children:"Google Text-to-Speech: Question Reading"}),"\n",(0,o.jsx)(t.p,{children:"After generating the questions, Google\u2019s Text-to-Speech (TTS) API is employed to read the questions aloud to the user, making the experience interactive and accessible. The TTS system uses deep learning algorithms for speech synthesis to produce natural-sounding voices. The questions are read aloud, allowing users to engage with the content even if they prefer audio input or have difficulty reading."}),"\n",(0,o.jsx)(t.h2,{id:"search-algorithms",children:"Search Algorithms"}),"\n",(0,o.jsx)(t.p,{children:"Additionally, search algorithms may be involved in refining the user experience, such as allowing for autocomplete when searching for videos or retrieving relevant videos based on a user search. These algorithms utilize basic keyword matching and ranking systems to ensure users can easily find what they\u2019re looking for within the system."})]})}function h(e={}){const{wrapper:t}={...(0,a.R)(),...e.components};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(l,{...e})}):l(e)}}}]);